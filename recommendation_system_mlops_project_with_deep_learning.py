# -*- coding: utf-8 -*-
"""Recommendation System MLOps Project with Deep Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JP9TbjwQLO4IHO7_snjl8OAEBITs9sr6
    Author: Ashesh Deep
"""

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import tensorflow as tf
import keras
from keras import backend as K

import requests
import zipfile
import io

from modelstore import ModelStore
import json

DATASET_LINK='http://files.grouplens.org/datasets/movielens/ml-100k.zip'

r = requests.get(DATASET_LINK)
z = zipfile.ZipFile(io.BytesIO(r.content))
z.extractall()

u_info = pd.read_csv('ml-100k/u.info', header=None)
print(u_info)

u_data_str = 'user id | movie id | rating | timestamp'
u_data_headers = u_data_str.split(' | ')

u_data_dataset = pd.read_csv('ml-100k/u.data', sep='\t',header=None,names=u_data_headers)
print(u_data_dataset.head())

len(u_data_dataset), max(u_data_dataset['movie id']),max(u_data_dataset['movie id'])

u_item_str = 'movie id | movie title | release date | video release date | IMDb URL | unknown | Action | Adventure | Animation | Children | Comedy | Crime | Documentary | Drama | Fantasy | Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | Western'
u_item_headers = u_item_str.split(' | ')
print(u_item_headers)

items_dataset = pd.read_csv('ml-100k/u.item', sep='|',header=None,names=u_item_headers,encoding='latin-1')
movie_dataset = items_dataset[['movie id','movie title']]
movie_dataset.head()

combined_dataset = pd.merge(u_data_dataset, movie_dataset, how='inner', on='movie id')

master_dataset = combined_dataset.groupby(by=['user id','movie title'], as_index=False).agg({"rating":"mean"})
master_dataset

user_encoder = LabelEncoder()
master_dataset['user'] = user_encoder.fit_transform(master_dataset['user id'].values)
n_users = master_dataset['user'].nunique()

movie_encoder = LabelEncoder()
master_dataset['movie'] = movie_encoder.fit_transform(master_dataset['movie title'].values)
n_movies = master_dataset['movie'].nunique()

master_dataset['rating'] = master_dataset['rating'].values.astype(np.float32)
min_rating = min(master_dataset['rating'])
max_rating = max(master_dataset['rating'])
n_users, n_movies, min_rating, max_rating

master_dataset.head()


X = master_dataset[['user', 'movie']].values
y = master_dataset['rating'].values

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=50)
x_train.shape, x_test.shape, y_train.shape, y_test.shape

n_factors = 150

x_train_mat = [x_train[:, 0], x_train[:, 1]]
x_test_mat = [x_test[:, 0], x_test[:, 1]]

x_train, x_train_mat, x_train_mat[0].shape

# Normalizing labels

y_train = (y_train - min_rating)/(max_rating - min_rating)
y_test = (y_test - min_rating)/(max_rating - min_rating)

# Building a Softmax Deep Neural Network

## Initializing a input layer for users
user = tf.keras.layers.Input(shape = (1,))

# Embedding layer for n_factors of users
u = keras.layers.embeddings.Embedding(n_users, n_factors, embeddings_initializer = 'he_normal', embeddings_regularizer = tf.keras.regularizers.l2(1e-6))(user)
u = tf.keras.layers.Reshape((n_factors,))(u)

#Input layers for movies
movie = tf.keras.layers.Input(shape = (1,))

## Embedding layer for n_factors of movies
m = keras.layers.embeddings.Embedding(n_movies, n_factors, embeddings_initializer = 'he_normal', embeddings_regularizer=tf.keras.regularizers.l2(1e-6))(movie)
m = tf.keras.layers.Reshape((n_factors,))(m)

x = tf.keras.layers.Concatenate()([u,m])
x = tf.keras.layers.Dropout(0.05)(x)

# Adding a Dense layer to the architecture
# Activation ReLu
x = tf.keras.layers.Dense(32, kernel_initializer='he_normal')(x)
x = tf.keras.layers.Activation(activation='relu')(x)
x = tf.keras.layers.Dropout(0.05)(x)

# Activation ReLu
x = tf.keras.layers.Dense(16, kernel_initializer='he_normal')(x)
x = tf.keras.layers.Activation(activation='relu')(x)
x = tf.keras.layers.Dropout(0.05)(x)

#Softmax Activation
x = tf.keras.layers.Dense(9)(x)
x = tf.keras.layers.Activation(activation='softmax')(x)

## Defining the model
model = tf.keras.models.Model(inputs=[user,movie], outputs=x)
model.compile(optimizer='sgd', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])


# from IPython.display import SVG
# from keras.utils.vis_utils import model_to_dot
# SVG(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))
# model.summary()

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.75, patience=3, min_lr=0.000001, verbose=1)
data = model.fit(x = x_train_mat, y = y_train, batch_size=128, epochs=2, verbose=1, validation_data=(x_test_mat, y_test) ,shuffle=True,callbacks=[reduce_lr])

model_store = ModelStore.from_aws_s3("iiscmlops")

domain = "prod-movie-model"
meta_data = model_store.upload(domain, model=model)

print(json.dumps(meta_data, indent=4))

print ("Model saved successfully in S3.")

